{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Home Assignment AA (2022/2023)\n",
    "### Group: 59"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825 739\n"
     ]
    }
   ],
   "source": [
    "biog = pd.read_csv(\"biodegradable_a.csv\", sep=\",\")\n",
    "\n",
    "X = biog.values[:, 0:41]\n",
    "y = biog[\"Biodegradable\"].values\n",
    "\n",
    "y[y == \"RB\"] = 1.0\n",
    "y[y == \"NRB\"] = 0.0\n",
    "\n",
    "X_train_IVS, X_IVS, y_train_IVS, y_IVS = train_test_split(X, y.astype(float), test_size=0.25, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_IVS, y_train_IVS, test_size=0.25, random_state=0)\n",
    "\n",
    "X_cols = biog.columns[0:41]\n",
    "\n",
    "countRB = 0\n",
    "countNRB = 0\n",
    "for i in y:\n",
    "    if(i == 1): countRB += 1\n",
    "    elif(i == 0): countNRB += 1\n",
    "\n",
    "print(countRB, countNRB)\n",
    "\n",
    "# pd.DataFrame(X, columns=X_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar valores em falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows original dataset = 4564\n",
      "No. of rows without NaN = 889 (19%)\n",
      "Threshold number of missing values per column = 685 (15%)\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values in each column.\n",
    "\n",
    "# dataset without lines with NaN values\n",
    "biog_nan = biog.dropna()\n",
    "#print(len(biog_nan))\n",
    "\n",
    "print(\"No. of rows original dataset = {}\".format(len(biog)))\n",
    "print(\"No. of rows without NaN = {} ({}%)\".format(len(biog_nan), round(len(biog_nan) / len(biog) * 100)))\n",
    "\n",
    "\n",
    "# check number of missing values in each column.\n",
    "missValues = biog.isna().sum()\n",
    "\n",
    "missValues = missValues.to_frame()\n",
    "print(\"Threshold number of missing values per column = {} (15%)\".format(str(round(0.15 * len(X)))))\n",
    "\n",
    "# print(missValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar os dados (Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo de Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "useMean = [0, 1, 7, 11, 12, 13, 14, 21, 26, 27, 38]\n",
    "useMedian = np.setdiff1d(range(41), useMean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# strategy = mean or median (for numerical data)\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_scaled)\n",
    "\n",
    "X_train_processed = ct.transform(X_train)\n",
    "X_test_processed = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SdssC      -0.497498\n",
      "nHDon      -0.453868\n",
      "nN_N       -0.451105\n",
      "F02_CN     -0.438829\n",
      "LOC        -0.420888\n",
      "Psi_i_1d   -0.411507\n",
      "F03        -0.400766\n",
      "SM6_B      -0.389450\n",
      "F03_CO     -0.375346\n",
      "SpMax_L    -0.364687\n",
      "Name: 0, dtype: float64\n",
      "The 10 selected features are: ['SdssC', 'nHDon', 'nN_N', 'F02_CN', 'LOC', 'Psi_i_1d', 'F03', 'SM6_B', 'F03_CO', 'SpMax_L']\n"
     ]
    }
   ],
   "source": [
    "N, M = X_train.shape\n",
    "\n",
    "v = np.hstack((y_train.reshape((N, 1)), X_train_processed))\n",
    "\n",
    "pd.DataFrame(v)\n",
    "corrMatrix = pd.DataFrame((np.corrcoef(v.T.astype(float))), columns=X_cols.insert(0, \"Y\")).iloc[0].sort_values()[0:10]\n",
    "\n",
    "selectedCorr = list(corrMatrix.index)\n",
    "\n",
    "print(corrMatrix)\n",
    "print(\"The 10 selected features are:\", selectedCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  SpMax_L | Importance:  0.060494089426139916\n",
      "Feature:  C | Importance:  0.03566003734354663\n",
      "Feature:  F03 | Importance:  0.039737927486255605\n",
      "Feature:  SdssC | Importance:  0.0994382022658157\n",
      "Feature:  LOC | Importance:  0.06993717149088713\n",
      "Feature:  SM6_L | Importance:  0.052890281681962935\n",
      "Feature:  F03_CO | Importance:  0.05433221712269346\n",
      "Feature:  nN_N | Importance:  0.04517668194461442\n",
      "Feature:  nHDon | Importance:  0.07324048446138173\n",
      "Feature:  nX | Importance:  0.049786795204055975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(), max_features=10)\n",
    "sel.fit(X_train_processed, y_train)\n",
    "\n",
    "selectedForest = []\n",
    "selectedForestIndixes = []\n",
    "\n",
    "for feature_list_index in sel.get_support(indices=True):\n",
    "    selectedForestIndixes.append(feature_list_index)\n",
    "    selectedForest.append(X_cols[feature_list_index])\n",
    "    print(\"Feature: \", X_cols[feature_list_index], \"| Importance: \", sel.estimator_.feature_importances_[feature_list_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SdssC', 'SM6_L', 'F03_CO', 'nN_N', 'nArNO2', 'B01', 'C_026', 'nHDon',\n",
      "       'SM6_B', 'nX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "N,M = X_train_processed.shape\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"forward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_for_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_for_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (BackWard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['F01', 'C', 'SdssC', 'SM6_L', 'F03_CO', 'nN_N', 'SpPosA_B', 'nCrt',\n",
      "       'C_026', 'nHDon'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"backward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_back_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_back_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes of Features Selcted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 11, 13, 15, 18, 27, 33, 34, 38]\n",
      "[0, 7, 10, 11, 13, 14, 15, 18, 34, 40]\n",
      "[11, 14, 15, 18, 19, 23, 32, 34, 38, 40]\n",
      "[3, 7, 11, 14, 15, 18, 21, 31, 32, 34]\n"
     ]
    }
   ],
   "source": [
    "corr_index = []\n",
    "rand_forest_index = selectedForestIndixes\n",
    "step_for_index = list(ST_for_selected_features)\n",
    "step_back_index = list(ST_back_selected_features)\n",
    "\n",
    "count = 0\n",
    "for col in X_cols:\n",
    "    if col in selectedCorr:\n",
    "        corr_index.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(corr_index)\n",
    "print(rand_forest_index)\n",
    "print(step_for_index)\n",
    "print(step_back_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_feat_indexes = [corr_index,\n",
    "                      rand_forest_index,\n",
    "                      step_for_index,\n",
    "                      step_back_index, \n",
    "                      range(0, 41)]\n",
    "\n",
    "selec_feat_names = [\"Correlation\",\n",
    "                      \"Random Forest\",\n",
    "                      \"StepWise For\",\n",
    "                      \"StepWise Back\", \n",
    "                      \"No Feature Selection\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar metricas de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.7682 with the hyperparameters {'neighbors': 18, 'weigths': <function gaussian at 0x00000268D220AEE0>, 'f1Score': 0.9649965682910089}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.7780 with the hyperparameters {'neighbors': 2, 'weigths': <function gaussian at 0x00000268D220AEE0>, 'f1Score': 0.9654218533886584}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7447 with the hyperparameters {'neighbors': 6, 'weigths': 'uniform', 'f1Score': 0.9626104690686608}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.7384 with the hyperparameters {'neighbors': 16, 'weigths': <function gaussian at 0x00000268D220AEE0>, 'f1Score': 0.9621621621621621}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.6635 with the hyperparameters {'neighbors': 2, 'weigths': 'uniform', 'f1Score': 0.9500342231348392}\n",
      "\n",
      "Confusion Matrix of best KNN model\n",
      " [[107  30]\n",
      " [ 47 672]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import sys\n",
    "\n",
    "\n",
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights\n",
    "\n",
    "for i in range(5):\n",
    "    # print(i)\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "            \"neighbors\": 0,\n",
    "            \"weigths\": 0,\n",
    "            \"f1Score\": 0\n",
    "    }\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    for k in range(2, 20, 2):\n",
    "        for wei in [gaussian, \"distance\", \"uniform\"]:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, weights=wei)\n",
    "            knn.fit(X_train, y_train)\n",
    "            preds = knn.predict(X_test)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            if(mcc > bestMCC):\n",
    "                bestMCC = mcc\n",
    "                bestMCCParams[\"neighbors\"] = k\n",
    "                bestMCCParams[\"weigths\"] = wei\n",
    "                bestMCCParams[\"f1Score\"] = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST KNN MODEL\n",
    "X_train = X_train_processed[:, step_back_index]\n",
    "X_test = X_test_processed[:, step_back_index]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=bestMCCParams[\"neighbors\"], weights=bestMCCParams[\"weigths\"])\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "bestKNNCM = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix of best KNN model\\n\", bestKNNCM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.6361 with the hyperparameters {'max_depth': 14, 'min_impurity_decrease': 0.05, 'f1Score': 0.946793997271487}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.6627 with the hyperparameters {'max_depth': 8, 'min_impurity_decrease': 0.05, 'f1Score': 0.9519945909398242}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.6143 with the hyperparameters {'max_depth': 10, 'min_impurity_decrease': 0.05, 'f1Score': 0.9472981987991994}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.6868 with the hyperparameters {'max_depth': 4, 'min_impurity_decrease': 0.05, 'f1Score': 0.9559412550066756}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.5790 with the hyperparameters {'max_depth': 18, 'min_impurity_decrease': 0.05, 'f1Score': 0.9438943894389439}\n",
      "\n",
      "Confusion Matrix of best RF model\n",
      " [[ 67  70]\n",
      " [  8 711]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "        bestMCC = sys.float_info.min\n",
    "        bestMCCParams = {\n",
    "                \"max_depth\": 0,\n",
    "                \"min_impurity_decrease\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "        print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "        X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "        X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "\n",
    "        for md in range(2, 20, 2):\n",
    "                for mid in np.arange(0.05, 0.2, 0.05):\n",
    "                        rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=md, min_impurity_decrease=mid)\n",
    "                        rf.fit(X_train, y_train)\n",
    "                        preds = rf.predict(X_test)\n",
    "                        f1Score = f1_score(y_test, preds) \n",
    "                        mcc = matthews_corrcoef(y_test, preds) \n",
    "                        \n",
    "                        if(mcc > bestMCC):\n",
    "                                bestMCC = mcc\n",
    "                                bestMCCParams[\"max_depth\"]= md\n",
    "                                bestMCCParams[\"min_impurity_decrease\"] = mid\n",
    "                                bestMCCParams[\"f1Score\"] = f1Score\n",
    "                                \n",
    "        \n",
    "        print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST RF MODEL\n",
    "X_train = X_train_processed[:, rand_forest_index]\n",
    "X_test = X_test_processed[:, rand_forest_index]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=bestMCCParams[\"max_depth\"], min_impurity_decrease=bestMCCParams[\"min_impurity_decrease\"])\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "bestRFCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best RF model\\n\", bestRFCM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.6372 with the hyperparameters {'n_estimators': 4, 'f1Score': 0.9415041782729805}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.6941 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.951388888888889}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7218 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9572413793103448}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.7269 with the hyperparameters {'n_estimators': 4, 'f1Score': 0.9579020013802623}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.7406 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9618528610354222}\n",
      "\n",
      "Confusion Matrix of best Bagging model\n",
      " [[100  37]\n",
      " [ 25 694]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "                \"n_estimators\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "    for n in range(2, 10, 2):\n",
    "        bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=n, random_state=22)\n",
    "        bag.fit(X_train, y_train)\n",
    "        preds = bag.predict(X_test)\n",
    "        mcc = matthews_corrcoef(y_test, preds)\n",
    "\n",
    "        if(bestMCC < mcc):\n",
    "            bestMCC = mcc\n",
    "            bestMCCParams[\"n_estimators\"] = n\n",
    "            bestMCCParams[\"f1Score\"] =  f1_score(y_test, preds)\n",
    "\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST Bagging MODEL\n",
    "X_train = X_train_processed[:, step_for_index]\n",
    "X_test = X_test_processed[:, step_for_index]\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=bestMCCParams[\"n_estimators\"], random_state=22)\n",
    "bag.fit(X_train, y_train)\n",
    "preds = bag.predict(X_test)\n",
    "\n",
    "bestBGCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best Bagging model\\n\", bestBGCM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and validating the best model \n",
    "\n",
    "(KNN | Step Back | Gaussian | k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Selected Model is a KNN, where k=8, using features selected with a Backwards StepWise and a Gaussian weight function: \n",
      "\n",
      "The MCC is:  0.8614892016858953\n",
      "The Confusion Matrix is: \n",
      " [[157  29]\n",
      " [ 13 942]]\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train_processed[:, step_back_index]\n",
    "# X_test = X_test_processed[:, step_back_index]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_IVS)\n",
    "\n",
    "X_train_IVS_scaled = scaler.transform(X_train_IVS)\n",
    "X_IVS_scaled = scaler.transform(X_IVS)\n",
    "\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_IVS_scaled)\n",
    "\n",
    "X_train_IVS_processed = ct.transform(X_train_IVS_scaled)\n",
    "X_IVS_processed = ct.transform(X_IVS_scaled)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8, weights=gaussian)\n",
    "knn.fit(X_train_IVS_processed, y_train_IVS)\n",
    "\n",
    "preds = knn.predict(X_IVS_processed)\n",
    "\n",
    "mcc = matthews_corrcoef(y_IVS, preds)\n",
    "cm = confusion_matrix(y_IVS, preds)\n",
    "\n",
    "print(\"Our Selected Model is a KNN, where k=8, using features selected with a Backwards StepWise and a Gaussian weight function: \\n\")\n",
    "print(\"The MCC is: \", mcc)\n",
    "print(\"The Confusion Matrix is: \\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "04e3edcbff521d4507541a888d0dd9335f64985a674087566b9b4c8f92f4be6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
