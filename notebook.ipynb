{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Home Assignment AA (2022/2023)\n",
    "### Group: 59"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825 739\n"
     ]
    }
   ],
   "source": [
    "biog = pd.read_csv(\"biodegradable_a.csv\", sep=\",\")\n",
    "\n",
    "X = biog.values[:, 0:41]\n",
    "y = biog[\"Biodegradable\"].values\n",
    "\n",
    "y[y == \"RB\"] = 1.0\n",
    "y[y == \"NRB\"] = 0.0\n",
    "\n",
    "X_train_IVS, X_IVS, y_train_IVS, y_IVS = train_test_split(X, y.astype(float), test_size=0.25, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_IVS, y_train_IVS, test_size=0.25, random_state=0)\n",
    "\n",
    "X_cols = biog.columns[0:41]\n",
    "\n",
    "countRB = 0\n",
    "countNRB = 0\n",
    "for i in y:\n",
    "    if(i == 1): countRB += 1\n",
    "    elif(i == 0): countNRB += 1\n",
    "\n",
    "print(countRB, countNRB)\n",
    "\n",
    "# pd.DataFrame(X, columns=X_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar valores em falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows original dataset = 4564\n",
      "No. of rows without NaN = 889 (19%)\n",
      "Threshold number of missing values per column = 685 (15%)\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values in each column.\n",
    "\n",
    "# dataset without lines with NaN values\n",
    "biog_nan = biog.dropna()\n",
    "#print(len(biog_nan))\n",
    "\n",
    "print(\"No. of rows original dataset = {}\".format(len(biog)))\n",
    "print(\"No. of rows without NaN = {} ({}%)\".format(len(biog_nan), round(len(biog_nan) / len(biog) * 100)))\n",
    "\n",
    "\n",
    "# check number of missing values in each column.\n",
    "missValues = biog.isna().sum()\n",
    "\n",
    "missValues = missValues.to_frame()\n",
    "print(\"Threshold number of missing values per column = {} (15%)\".format(str(round(0.15 * len(X)))))\n",
    "\n",
    "# print(missValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar os dados (Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo de Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "useMean = [0, 1, 7, 11, 12, 13, 14, 21, 26, 27, 38]\n",
    "useMedian = np.setdiff1d(range(41), useMean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# strategy = mean or median (for numerical data)\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_scaled)\n",
    "\n",
    "X_train_processed = ct.transform(X_train)\n",
    "X_test_processed = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SdssC      -0.497498\n",
      "nHDon      -0.453868\n",
      "nN_N       -0.451105\n",
      "F02_CN     -0.438829\n",
      "LOC        -0.420888\n",
      "Psi_i_1d   -0.411507\n",
      "F03        -0.400766\n",
      "SM6_B      -0.389450\n",
      "F03_CO     -0.375346\n",
      "SpMax_L    -0.364687\n",
      "Name: 0, dtype: float64\n",
      "The 10 selected features are: ['SdssC', 'nHDon', 'nN_N', 'F02_CN', 'LOC', 'Psi_i_1d', 'F03', 'SM6_B', 'F03_CO', 'SpMax_L']\n"
     ]
    }
   ],
   "source": [
    "N, M = X_train.shape\n",
    "\n",
    "v = np.hstack((y_train.reshape((N, 1)), X_train_processed))\n",
    "\n",
    "pd.DataFrame(v)\n",
    "corrMatrix = pd.DataFrame((np.corrcoef(v.T.astype(float))), columns=X_cols.insert(0, \"Y\")).iloc[0].sort_values()[0:10]\n",
    "\n",
    "selectedCorr = list(corrMatrix.index)\n",
    "\n",
    "print(corrMatrix)\n",
    "print(\"The 10 selected features are:\", selectedCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  SpMax_L | Importance:  0.05446797911305484\n",
      "Feature:  C | Importance:  0.03443021592182938\n",
      "Feature:  F03 | Importance:  0.040435473803016084\n",
      "Feature:  SdssC | Importance:  0.08519496192498861\n",
      "Feature:  LOC | Importance:  0.06943558237665579\n",
      "Feature:  SM6_L | Importance:  0.054891207651519246\n",
      "Feature:  F03_CO | Importance:  0.05428828513681049\n",
      "Feature:  nN_N | Importance:  0.061660318099339036\n",
      "Feature:  nHDon | Importance:  0.05362346458992875\n",
      "Feature:  nX | Importance:  0.05717782814562563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(), max_features=10)\n",
    "sel.fit(X_train_processed, y_train)\n",
    "\n",
    "selectedForest = []\n",
    "selectedForestIndixes = []\n",
    "\n",
    "for feature_list_index in sel.get_support(indices=True):\n",
    "    selectedForestIndixes.append(feature_list_index)\n",
    "    selectedForest.append(X_cols[feature_list_index])\n",
    "    print(\"Feature: \", X_cols[feature_list_index], \"| Importance: \", sel.estimator_.feature_importances_[feature_list_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SdssC', 'SM6_L', 'F03_CO', 'nN_N', 'nArNO2', 'B01', 'B03', 'C_026',\n",
      "       'nHDon', 'nX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "N,M = X_train_processed.shape\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"forward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_for_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_for_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (BackWard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['F01', 'NssssC', 'C', 'SdssC', 'LOC', 'SM6_L', 'F03_CO', 'Me', 'nN_N',\n",
      "       'SpMax_B'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"backward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_back_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_back_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes of Features Selcted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 11, 13, 15, 18, 27, 33, 34, 38]\n",
      "[0, 7, 10, 11, 13, 14, 15, 18, 34, 40]\n",
      "[11, 14, 15, 18, 19, 23, 24, 32, 34, 40]\n",
      "[3, 5, 7, 11, 13, 14, 15, 16, 18, 35]\n"
     ]
    }
   ],
   "source": [
    "corr_index = []\n",
    "rand_forest_index = selectedForestIndixes\n",
    "step_for_index = list(ST_for_selected_features)\n",
    "step_back_index = list(ST_back_selected_features)\n",
    "\n",
    "count = 0\n",
    "for col in X_cols:\n",
    "    if col in selectedCorr:\n",
    "        corr_index.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(corr_index)\n",
    "print(rand_forest_index)\n",
    "print(step_for_index)\n",
    "print(step_back_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_feat_indexes = [corr_index,\n",
    "                      rand_forest_index,\n",
    "                      step_for_index,\n",
    "                      step_back_index, \n",
    "                      range(0, 41)]\n",
    "\n",
    "selec_feat_names = [\"Correlation\",\n",
    "                      \"Random Forest\",\n",
    "                      \"StepWise For\",\n",
    "                      \"StepWise Back\", \n",
    "                      \"No Feature Selection\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar metricas de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.7568 with the hyperparameters {'neighbors': 5, 'weigths': <function gaussian at 0x00000268D220A3A0>, 'f1Score': 0.9628}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.7649 with the hyperparameters {'neighbors': 3, 'weigths': <function gaussian at 0x00000268D220A3A0>, 'f1Score': 0.9642}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7479 with the hyperparameters {'neighbors': 7, 'weigths': 'distance', 'f1Score': 0.9624}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.7538 with the hyperparameters {'neighbors': 3, 'weigths': 'uniform', 'f1Score': 0.9641}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.6425 with the hyperparameters {'neighbors': 3, 'weigths': <function gaussian at 0x00000268D220A3A0>, 'f1Score': 0.9493}\n",
      "\n",
      "Confusion Matrix of best KNN model\n",
      " [[ 91  46]\n",
      " [  9 710]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import sys\n",
    "\n",
    "\n",
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights\n",
    "\n",
    "for i in range(5):\n",
    "    # print(i)\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "            \"neighbors\": 0,\n",
    "            \"weigths\": 0,\n",
    "            \"f1Score\": 0\n",
    "    }\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    for k in range(3, 11, 2):\n",
    "        for wei in [gaussian, \"distance\", \"uniform\"]:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, weights=wei)\n",
    "            knn.fit(X_train, y_train)\n",
    "            preds = knn.predict(X_test)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            if(mcc > bestMCC):\n",
    "                bestMCC = mcc\n",
    "                bestMCCParams[\"neighbors\"] = k\n",
    "                bestMCCParams[\"weigths\"] = wei\n",
    "                bestMCCParams[\"f1Score\"] = round(f1_score(y_test, preds), 4)\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST KNN MODEL\n",
    "X_train = X_train_processed[:, step_back_index]\n",
    "X_test = X_test_processed[:, step_back_index]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=bestMCCParams[\"neighbors\"], weights=bestMCCParams[\"weigths\"])\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "bestKNNCM = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix of best KNN model\\n\", bestKNNCM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.6321 with the hyperparameters {'max_depth': 16, 'min_impurity_decrease': 0.05, 'f1Score': 0.9461}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.6925 with the hyperparameters {'max_depth': 10, 'min_impurity_decrease': 0.05, 'f1Score': 0.955}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.6404 with the hyperparameters {'max_depth': 12, 'min_impurity_decrease': 0.05, 'f1Score': 0.9484}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.6644 with the hyperparameters {'max_depth': 4, 'min_impurity_decrease': 0.05, 'f1Score': 0.9531}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.5743 with the hyperparameters {'max_depth': 14, 'min_impurity_decrease': 0.05, 'f1Score': 0.9434}\n",
      "\n",
      "Confusion Matrix of best RF model\n",
      " [[ 75  62]\n",
      " [ 14 705]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "        bestMCC = sys.float_info.min\n",
    "        bestMCCParams = {\n",
    "                \"max_depth\": 0,\n",
    "                \"min_impurity_decrease\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "        print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "        X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "        X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "\n",
    "        for md in range(2, 20, 2):\n",
    "                for mid in np.arange(0.05, 0.2, 0.05):\n",
    "                        rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=md, min_impurity_decrease=mid)\n",
    "                        rf.fit(X_train, y_train)\n",
    "                        preds = rf.predict(X_test)\n",
    "                        f1Score = round(f1_score(y_test, preds), 4)\n",
    "                        mcc = matthews_corrcoef(y_test, preds) \n",
    "                        \n",
    "                        if(mcc > bestMCC):\n",
    "                                bestMCC = mcc\n",
    "                                bestMCCParams[\"max_depth\"]= md\n",
    "                                bestMCCParams[\"min_impurity_decrease\"] = mid\n",
    "                                bestMCCParams[\"f1Score\"] = f1Score\n",
    "                                \n",
    "        \n",
    "        print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST RF MODEL\n",
    "X_train = X_train_processed[:, rand_forest_index]\n",
    "X_test = X_test_processed[:, rand_forest_index]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=bestMCCParams[\"max_depth\"], min_impurity_decrease=bestMCCParams[\"min_impurity_decrease\"])\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "bestRFCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best RF model\\n\", bestRFCM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.6372 with the hyperparameters {'n_estimators': 4, 'f1Score': 0.9415}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.6941 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9514}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7024 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9575}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.6800 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9502}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.7406 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9619}\n",
      "\n",
      "Confusion Matrix of best Bagging model\n",
      " [[ 83  54]\n",
      " [  9 710]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "                \"n_estimators\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "    for n in range(2, 10, 2):\n",
    "        bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=n, random_state=22)\n",
    "        bag.fit(X_train, y_train)\n",
    "        preds = bag.predict(X_test)\n",
    "        mcc = matthews_corrcoef(y_test, preds)\n",
    "\n",
    "        if(bestMCC < mcc):\n",
    "            bestMCC = mcc\n",
    "            bestMCCParams[\"n_estimators\"] = n\n",
    "            bestMCCParams[\"f1Score\"] =  round(f1_score(y_test, preds), 4)\n",
    "\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST Bagging MODEL\n",
    "X_train = X_train_processed[:, step_for_index]\n",
    "X_test = X_test_processed[:, step_for_index]\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=bestMCCParams[\"n_estimators\"], random_state=22)\n",
    "bag.fit(X_train, y_train)\n",
    "preds = bag.predict(X_test)\n",
    "\n",
    "bestBGCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best Bagging model\\n\", bestBGCM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and validating the best model \n",
    "\n",
    "(KNN | Random Forest Feature Selection | Gaussian | k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Selected Model is a KNN, where k=8, using features selected with a Backwards StepWise and a Gaussian weight function: \n",
      "\n",
      "The MCC is:  0.8184\n",
      "The Confusion Matrix is: \n",
      " [[151  35]\n",
      " [ 20 935]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_IVS)\n",
    "\n",
    "X_train_IVS_scaled = scaler.transform(X_train_IVS)\n",
    "X_IVS_scaled = scaler.transform(X_IVS)\n",
    "\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_IVS_scaled)\n",
    "\n",
    "X_train_IVS_processed = ct.transform(X_train_IVS_scaled)[:, rand_forest_index]\n",
    "X_IVS_processed = ct.transform(X_IVS_scaled)[:, rand_forest_index]\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights=gaussian)\n",
    "knn.fit(X_train_IVS_processed, y_train_IVS)\n",
    "\n",
    "preds = knn.predict(X_IVS_processed)\n",
    "\n",
    "mcc = matthews_corrcoef(y_IVS, preds)\n",
    "cm = confusion_matrix(y_IVS, preds)\n",
    "\n",
    "print(\"Our Selected Model is a KNN, where k=8, using features selected with a Backwards StepWise and a Gaussian weight function: \\n\")\n",
    "print(\"The MCC is: \", round(mcc, 4))\n",
    "print(\"The Confusion Matrix is: \\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "04e3edcbff521d4507541a888d0dd9335f64985a674087566b9b4c8f92f4be6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
