{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Home Assignment AA (2022/2023)\n",
    "### Group: 59"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825 739\n"
     ]
    }
   ],
   "source": [
    "biog = pd.read_csv(\"biodegradable_a.csv\", sep=\",\")\n",
    "\n",
    "X = biog.values[:, 0:41]\n",
    "y = biog[\"Biodegradable\"].values\n",
    "\n",
    "y[y == \"RB\"] = 1.0\n",
    "y[y == \"NRB\"] = 0.0\n",
    "\n",
    "X_train_IVS, X_IVS, y_train_IVS, y_IVS = train_test_split(X, y.astype(float), test_size=0.33, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_IVS, y_train_IVS, test_size=0.33, random_state=0)\n",
    "\n",
    "X_cols = biog.columns[0:41]\n",
    "\n",
    "countRB = 0\n",
    "countNRB = 0\n",
    "for i in y:\n",
    "    if(i == 1): countRB += 1\n",
    "    elif(i == 0): countNRB += 1\n",
    "\n",
    "print(countRB, countNRB)\n",
    "\n",
    "# pd.DataFrame(X, columns=X_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar valores em falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows original dataset = 4564\n",
      "No. of rows without NaN = 889 (19%)\n",
      "Threshold number of missing values per column = 685 (15%)\n"
     ]
    }
   ],
   "source": [
    "# check number of missing values in each column.\n",
    "\n",
    "# dataset without lines with NaN values\n",
    "biog_nan = biog.dropna()\n",
    "#print(len(biog_nan))\n",
    "\n",
    "print(\"No. of rows original dataset = {}\".format(len(biog)))\n",
    "print(\"No. of rows without NaN = {} ({}%)\".format(len(biog_nan), round(len(biog_nan) / len(biog) * 100)))\n",
    "\n",
    "\n",
    "# check number of missing values in each column.\n",
    "missValues = biog.isna().sum()\n",
    "\n",
    "missValues = missValues.to_frame()\n",
    "print(\"Threshold number of missing values per column = {} (15%)\".format(str(round(0.15 * len(X)))))\n",
    "\n",
    "# print(missValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar os dados (Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo de Imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "useMean = [0, 1, 7, 11, 12, 13, 14, 21, 26, 27, 38]\n",
    "useMedian = np.setdiff1d(range(41), useMean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# strategy = mean or median (for numerical data)\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_scaled)\n",
    "\n",
    "X_train_processed = ct.transform(X_train)\n",
    "X_test_processed = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SdssC      -0.493378\n",
      "nHDon      -0.459505\n",
      "nN_N       -0.447186\n",
      "F02_CN     -0.426871\n",
      "SM6_B      -0.418151\n",
      "LOC        -0.411746\n",
      "F03        -0.391805\n",
      "Psi_i_1d   -0.381357\n",
      "F03_CO     -0.354265\n",
      "SpMax_L    -0.344159\n",
      "Name: 0, dtype: float64\n",
      "The 10 selected features are: ['SdssC', 'nHDon', 'nN_N', 'F02_CN', 'SM6_B', 'LOC', 'F03', 'Psi_i_1d', 'F03_CO', 'SpMax_L']\n"
     ]
    }
   ],
   "source": [
    "N, M = X_train.shape\n",
    "\n",
    "v = np.hstack((y_train.reshape((N, 1)), X_train_processed))\n",
    "\n",
    "pd.DataFrame(v)\n",
    "corrMatrix = pd.DataFrame((np.corrcoef(v.T.astype(float))), columns=X_cols.insert(0, \"Y\")).iloc[0].sort_values()[0:10]\n",
    "\n",
    "selectedCorr = list(corrMatrix.index)\n",
    "\n",
    "print(corrMatrix)\n",
    "print(\"The 10 selected features are:\", selectedCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  SpMax_L | Importance:  0.06690662116058628\n",
      "Feature:  C | Importance:  0.051812353914987196\n",
      "Feature:  F03 | Importance:  0.050345870873060146\n",
      "Feature:  SdssC | Importance:  0.08674208590421663\n",
      "Feature:  LOC | Importance:  0.052008967071320064\n",
      "Feature:  SM6_L | Importance:  0.0524074184687339\n",
      "Feature:  F03_CO | Importance:  0.035343463030889824\n",
      "Feature:  nN_N | Importance:  0.05649352387372817\n",
      "Feature:  nHDon | Importance:  0.06283164567733486\n",
      "Feature:  nX | Importance:  0.041586306372976695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(), max_features=10)\n",
    "sel.fit(X_train_processed, y_train)\n",
    "\n",
    "selectedForest = []\n",
    "selectedForestIndixes = []\n",
    "\n",
    "for feature_list_index in sel.get_support(indices=True):\n",
    "    selectedForestIndixes.append(feature_list_index)\n",
    "    selectedForest.append(X_cols[feature_list_index])\n",
    "    print(\"Feature: \", X_cols[feature_list_index], \"| Importance: \", sel.estimator_.feature_importances_[feature_list_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SdssC', 'LOC', 'SM6_L', 'F03_CO', 'nArNO2', 'B03', 'C_026', 'F02_CN',\n",
      "       'nHDon', 'nX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "N,M = X_train_processed.shape\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"forward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_for_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_for_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepWise (BackWard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['F01', 'NssssC', 'C', 'F03', 'SdssC', 'LOC', 'SM6_L', 'F03_CO', 'TI2_L',\n",
      "       'nHDon'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "sfs = SequentialFeatureSelector(dtc, n_features_to_select=10, direction=\"backward\")\n",
    "\n",
    "sfs.fit(X_train_processed, y_train)\n",
    "\n",
    "features = sfs.get_support()\n",
    "ST_back_selected_features = np.arange(M)[features]\n",
    "\n",
    "print(X_cols[ST_back_selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes of Features Selcted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 11, 13, 15, 18, 27, 33, 34, 38]\n",
      "[0, 7, 10, 11, 13, 14, 15, 18, 34, 40]\n",
      "[11, 13, 14, 15, 19, 24, 32, 33, 34, 40]\n",
      "[3, 5, 7, 10, 11, 13, 14, 15, 30, 34]\n"
     ]
    }
   ],
   "source": [
    "corr_index = []\n",
    "rand_forest_index = selectedForestIndixes\n",
    "step_for_index = list(ST_for_selected_features)\n",
    "step_back_index = list(ST_back_selected_features)\n",
    "\n",
    "count = 0\n",
    "for col in X_cols:\n",
    "    if col in selectedCorr:\n",
    "        corr_index.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(corr_index)\n",
    "print(rand_forest_index)\n",
    "print(step_for_index)\n",
    "print(step_back_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_feat_indexes = [corr_index,\n",
    "                      rand_forest_index,\n",
    "                      step_for_index,\n",
    "                      step_back_index, \n",
    "                      range(0, 41)]\n",
    "\n",
    "selec_feat_names = [\"Correlation\",\n",
    "                      \"Random Forest\",\n",
    "                      \"StepWise For\",\n",
    "                      \"StepWise Back\", \n",
    "                      \"No Feature Selection\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar metricas de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.7719 with the hyperparameters {'neighbors': 4, 'weigths': <function gaussian at 0x000001B313DF9F70>, 'f1Score': 0.9656377402446127}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.7711 with the hyperparameters {'neighbors': 12, 'weigths': 'distance', 'f1Score': 0.9665898617511521}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7863 with the hyperparameters {'neighbors': 8, 'weigths': <function gaussian at 0x000001B313DF9F70>, 'f1Score': 0.9680418361417781}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.8135 with the hyperparameters {'neighbors': 4, 'weigths': 'distance', 'f1Score': 0.9721577726218098}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.6470 with the hyperparameters {'neighbors': 2, 'weigths': 'uniform', 'f1Score': 0.9477351916376306}\n",
      "\n",
      "Confusion Matrix of best KNN model\n",
      " [[131  29]\n",
      " [ 31 818]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import sys\n",
    "\n",
    "\n",
    "def gaussian(dsts):\n",
    "    kernel_width = .5\n",
    "    weights = np.exp(-(dsts**2)/kernel_width)\n",
    "    return weights\n",
    "\n",
    "for i in range(5):\n",
    "    # print(i)\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "            \"neighbors\": 0,\n",
    "            \"weigths\": 0,\n",
    "            \"f1Score\": 0\n",
    "    }\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    for k in range(2, 20, 2):\n",
    "        for wei in [gaussian, \"distance\", \"uniform\"]:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, weights=wei)\n",
    "            knn.fit(X_train, y_train)\n",
    "            preds = knn.predict(X_test)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            if(mcc > bestMCC):\n",
    "                bestMCC = mcc\n",
    "                bestMCCParams[\"neighbors\"] = k\n",
    "                bestMCCParams[\"weigths\"] = wei\n",
    "                bestMCCParams[\"f1Score\"] = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST KNN MODEL\n",
    "X_train = X_train_processed[:, step_back_index]\n",
    "X_test = X_test_processed[:, step_back_index]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=bestMCCParams[\"neighbors\"], weights=bestMCCParams[\"weigths\"])\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "bestKNNCM = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix of best KNN model\\n\", bestKNNCM)\n",
    "\n",
    "            \n",
    "# plt.plot(range(2, 20, 2), f1gauss, label=\"gaussian\")\n",
    "# plt.plot(range(2, 20, 2), f1distance, label=\"distance\")\n",
    "# plt.plot(range(2, 20, 2), f1noWei, label=\"no weights\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.6309 with the hyperparameters {'max_depth': 6, 'min_impurity_decrease': 0.05, 'f1Score': 0.9477311889718552}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.7084 with the hyperparameters {'max_depth': 4, 'min_impurity_decrease': 0.05, 'f1Score': 0.9556074766355139}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.6619 with the hyperparameters {'max_depth': 14, 'min_impurity_decrease': 0.05, 'f1Score': 0.9516685845799769}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.6518 with the hyperparameters {'max_depth': 10, 'min_impurity_decrease': 0.05, 'f1Score': 0.9515116942384484}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.6134 with the hyperparameters {'max_depth': 14, 'min_impurity_decrease': 0.05, 'f1Score': 0.9480156512017887}\n",
      "\n",
      "Confusion Matrix of best RF model\n",
      " [[100  60]\n",
      " [ 25 824]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "        bestMCC = sys.float_info.min\n",
    "        bestMCCParams = {\n",
    "                \"max_depth\": 0,\n",
    "                \"min_impurity_decrease\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "        print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "        X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "        X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "\n",
    "        for md in range(2, 20, 2):\n",
    "                for mid in np.arange(0.05, 0.2, 0.05):\n",
    "                        rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=md, min_impurity_decrease=mid)\n",
    "                        rf.fit(X_train, y_train)\n",
    "                        preds = rf.predict(X_test)\n",
    "                        f1Score = f1_score(y_test, preds) \n",
    "                        mcc = matthews_corrcoef(y_test, preds) \n",
    "                        \n",
    "                        if(mcc > bestMCC):\n",
    "                                bestMCC = mcc\n",
    "                                bestMCCParams[\"max_depth\"]= md\n",
    "                                bestMCCParams[\"min_impurity_decrease\"] = mid\n",
    "                                bestMCCParams[\"f1Score\"] = f1Score\n",
    "                                \n",
    "        \n",
    "        print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST RF MODEL\n",
    "X_train = X_train_processed[:, rand_forest_index]\n",
    "X_test = X_test_processed[:, rand_forest_index]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", max_depth=bestMCCParams[\"max_depth\"], min_impurity_decrease=bestMCCParams[\"min_impurity_decrease\"])\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "bestRFCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best RF model\\n\", bestRFCM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection method: Correlation\n",
      "The best MCC we found was 0.5872 with the hyperparameters {'n_estimators': 6, 'f1Score': 0.9317507418397627}\n",
      "Feature selection method: Random Forest\n",
      "The best MCC we found was 0.6984 with the hyperparameters {'n_estimators': 8, 'f1Score': 0.9523248969982343}\n",
      "Feature selection method: StepWise For\n",
      "The best MCC we found was 0.7452 with the hyperparameters {'n_estimators': 4, 'f1Score': 0.9632606199770379}\n",
      "Feature selection method: StepWise Back\n",
      "The best MCC we found was 0.6945 with the hyperparameters {'n_estimators': 2, 'f1Score': 0.9502958579881656}\n",
      "Feature selection method: No Feature Selection\n",
      "The best MCC we found was 0.7635 with the hyperparameters {'n_estimators': 4, 'f1Score': 0.9653579676674365}\n",
      "\n",
      "Confusion Matrix of best Bagging model\n",
      " [[106  54]\n",
      " [ 10 839]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Feature selection method: \" + selec_feat_names[i])\n",
    "\n",
    "    X_train = X_train_processed[:, selec_feat_indexes[i]]\n",
    "    X_test = X_test_processed[:, selec_feat_indexes[i]]\n",
    "\n",
    "    bestMCC = sys.float_info.min\n",
    "    bestMCCParams = {\n",
    "                \"n_estimators\": 0,\n",
    "                \"f1Score\": 0\n",
    "        }\n",
    "\n",
    "    for n in range(2, 10, 2):\n",
    "        bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=n, random_state=22)\n",
    "        bag.fit(X_train, y_train)\n",
    "        preds = bag.predict(X_test)\n",
    "        mcc = matthews_corrcoef(y_test, preds)\n",
    "\n",
    "        if(bestMCC < mcc):\n",
    "            bestMCC = mcc\n",
    "            bestMCCParams[\"n_estimators\"] = n\n",
    "            bestMCCParams[\"f1Score\"] =  f1_score(y_test, preds)\n",
    "\n",
    "\n",
    "    print(f\"The best MCC we found was {'{:.4f}'.format(bestMCC)} with the hyperparameters {bestMCCParams}\")\n",
    "\n",
    "\n",
    "# FIT BEST Bagging MODEL\n",
    "X_train = X_train_processed[:, step_for_index]\n",
    "X_test = X_test_processed[:, step_for_index]\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=GaussianNB(), n_estimators=bestMCCParams[\"n_estimators\"], random_state=22)\n",
    "bag.fit(X_train, y_train)\n",
    "preds = bag.predict(X_test)\n",
    "\n",
    "bestBGCM = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix of best Bagging model\\n\", bestBGCM)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and validating the best model \n",
    "\n",
    "(KNN | Step Back | Gaussian | k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Selected Model is a KNN, where k=2, using features selected with a Backwards StepWise and a Gaussian weight function: \n",
      "\n",
      "The MCC is:  0.8483190422946583\n",
      "The Confusion Matrix is: \n",
      " [[ 208   46]\n",
      " [  16 1237]]\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train_processed[:, step_back_index]\n",
    "# X_test = X_test_processed[:, step_back_index]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_IVS)\n",
    "\n",
    "X_train_IVS_scaled = scaler.transform(X_train_IVS)\n",
    "X_IVS_scaled = scaler.transform(X_IVS)\n",
    "\n",
    "imputerMean = SimpleImputer(missing_values = np.nan, strategy = \"mean\") \n",
    "imputerMedian = SimpleImputer(missing_values = np.nan, strategy = \"median\") \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('mean_imputer', imputerMean, useMean),\n",
    "     ('median_imputer', imputerMedian, useMedian)]\n",
    ")\n",
    "\n",
    "ct.fit(X_train_IVS_scaled)\n",
    "\n",
    "X_train_IVS_processed = ct.transform(X_train_IVS_scaled)\n",
    "X_IVS_processed = ct.transform(X_IVS_scaled)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights=gaussian)\n",
    "knn.fit(X_train_IVS_processed, y_train_IVS)\n",
    "\n",
    "preds = knn.predict(X_IVS_processed)\n",
    "\n",
    "mcc = matthews_corrcoef(y_IVS, preds)\n",
    "cm = confusion_matrix(y_IVS, preds)\n",
    "\n",
    "print(\"Our Selected Model is a KNN, where k=2, using features selected with a Backwards StepWise and a Gaussian weight function: \\n\")\n",
    "print(\"The MCC is: \", mcc)\n",
    "print(\"The Confusion Matrix is: \\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "04e3edcbff521d4507541a888d0dd9335f64985a674087566b9b4c8f92f4be6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
